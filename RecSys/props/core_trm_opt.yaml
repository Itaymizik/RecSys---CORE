# COREtrmOpt Configuration
embedding_size: 64
n_layers: 2
n_heads: 2
max_seq_len: 20
inner_size: 256
hidden_dropout_prob: 0.5
attn_dropout_prob: 0.2
hidden_act: 'gelu'
layer_norm_eps: 1e-12
initializer_range: 0.02
loss_type: 'CE'

sess_dropout: 0.2
item_dropout: 0.2
temperature: 0.07

# Optimization
learning_rate: 0.001
weight_decay: 1e-4

# Phase 1 Improvements
label_smoothing: 0.1
enable_sam: true
sam_rho: 0.05
enable_swa: false  # Disable SWA for initial run as complexities with Trainer integration are high
