# Configuration for CORE-trm with Enhanced Positional Encoding
# Combines relative PE and context-aware position embeddings (CAPE-inspired)

# Model Architecture
embedding_size: 64
n_layers: 1
n_heads: 2
inner_size: 256

# Dropout Configuration
hidden_dropout_prob: 0.2
attn_dropout_prob: 0.2
sess_dropout: 0.2
item_dropout: 0.25

# Activation and Normalization
hidden_act: 'gelu'
layer_norm_eps: 1e-12
initializer_range: 0.02

# Enhanced Positional Encoding Parameters
use_relative_pe: True           # Enable relative positional encoding in attention
use_context_aware_pe: True      # Enable context-aware position embeddings (CAPE)
max_relative_position: 32       # Maximum relative distance to consider

# Temperature for RDM
temperature: 0.07

# Loss Configuration
loss_type: 'CE'
