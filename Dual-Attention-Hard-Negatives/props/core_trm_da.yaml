embedding_size: 100
inner_size: 256
n_layers: 2
n_heads: 2
hidden_dropout_prob: 0.5
attn_dropout_prob: 0.5
hidden_act: gelu
layer_norm_eps: 1e-12
initializer_range: 0.02
loss_type: CE

sess_dropout: 0.2
item_dropout: 0.2
temperature: 0.07

# Dual-attention options (Eq. 6-10 from the paper)
use_dual_gating: true
enable_recent_branch: true
enable_ave_residual: true

# Hard negatives (Eq. 13-15)
use_hard_negatives: true
hard_neg_k: 64
hard_neg_lambda: 0.2

